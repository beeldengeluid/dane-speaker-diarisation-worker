{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoZ59rEP-He6",
        "outputId": "57f207a6-14e9-4e24-ee15-7b3d926d8c6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyannote.audio\n",
        "!pip install pyannote.core\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install faster-whisper\n",
        "!pip install --no-build-isolation nemo_toolkit[asr]==1.22.0\n",
        "#!pip install wget"
      ],
      "metadata": {
        "id": "pqA8eiMW5Gay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hkKwlEXj2Res"
      },
      "outputs": [],
      "source": [
        "#segment\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import torch, torchaudio\n",
        "#import ffmpeg\n",
        "import os\n",
        "#import librosa\n",
        "\n",
        "#embed\n",
        "from faster_whisper import WhisperModel\n",
        "#from segment import segment_audio\n",
        "#from embed import embed_audio\n",
        "import tensorflow as tf\n",
        "\n",
        "#diarize\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "import wget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "h65tbh7NMW3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio = Audio(mono='downmix')\n",
        "\n",
        "def segmentation(trans_segment,\n",
        "            waveform,\n",
        "            #video,\n",
        "            sr,\n",
        "            seg_num,\n",
        "            segment_audio=True,\n",
        "            segment_video=False):\n",
        "\n",
        "    if segment_audio:\n",
        "        #segmenting\n",
        "        start = trans_segment.start\n",
        "        end = trans_segment.end\n",
        "        clip = Segment(start, end)\n",
        "        audio_segment, sr = audio.crop(waveform, clip) #as a tensor\n",
        "\n",
        "        #extracting file name and extension\n",
        "        #name, extension = os.path.splitext(waveform)\n",
        "        #if not os.path.exists(\"%s_segments\" % (name)):\n",
        "         #   os.makedirs(\"%s_segments\" % (name))\n",
        "\n",
        "        #saving segment into file\n",
        "        #torchaudio.save(\"./%s_segments/%s_seg_%x%s\" % (name, name, seg_num, extension),\n",
        "         #                audio_segment, sr, format=\"wav\")\n",
        "\n",
        "        return audio_segment"
      ],
      "metadata": {
        "id": "9tu_-TNH2aUw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "vnV0RBAUMZmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    # load the pre-trained checkpoints\n",
        "    model_checkpoint_loc = '/content/drive/MyDrive/master/data_internship/WavLM-Large.pt'\n",
        "    checkpoint = torch.load(model_checkpoint_loc) #should be .pt file\n",
        "    cfg = WavLMConfig(checkpoint['cfg'])\n",
        "    model = WavLM(cfg)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.eval()\n",
        "    return model, cfg\n",
        "\n",
        "def embed_audio(model, cfg, audio_segment):\n",
        "    #audio_segment = segment_audio(trans_segment, waveform, sr)\n",
        "    if cfg.normalize:\n",
        "        wav_input_16khz = torch.nn.functional.layer_norm(audio_segment, audio_segment.shape)\n",
        "    rep = model.extract_features(wav_input_16khz)[0]\n",
        "    return rep"
      ],
      "metadata": {
        "id": "rjV3BUCO2aWm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/master/data_internship')\n",
        "from WavLM import WavLM, WavLMConfig"
      ],
      "metadata": {
        "id": "L_oVIr8-I2oC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading feature extraction model\n",
        "model, cfg = load_model()"
      ],
      "metadata": {
        "id": "-JLk6vYsI_iK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "3fFe6J4OMfI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster-whisper Transcription"
      ],
      "metadata": {
        "id": "nx19gQrbMrv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = \"large-v3\"\n",
        "\n",
        "# Run on GPU with FP16\n",
        "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "# or run on GPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "# or run on CPU with INT8\n",
        "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
        "\n",
        "audioin = \"/content/drive/MyDrive/master/data_internship/testaudio.wav\"\n",
        "\n",
        "segments, info = model.transcribe(audioin, beam_size=5)\n",
        "\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))"
      ],
      "metadata": {
        "id": "5sVZEmil2aYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment and embed"
      ],
      "metadata": {
        "id": "yB2TstG9M5XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- PARAMS ---#\n",
        "#sampling rate\n",
        "sr = 16000\n",
        "#segment number initialization\n",
        "seg_num = 0\n",
        "\n",
        "for segment in segments:\n",
        "    #print(segment.text)\n",
        "    #calling segmentation creates\n",
        "    seg = segmentation(segment, audioin, sr, seg_num)\n",
        "    seg_num+=1\n",
        "    seg1= torch.randn(1,30000)\n",
        "    emb = embed_audio(model, cfg, seg)\n",
        "    print(emb)"
      ],
      "metadata": {
        "id": "6ftM5YYh2aax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d88b3b-244b-4590-9f7c-81b280a945b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4782, -0.0683, -0.3483,  ..., -0.1665,  0.0502, -0.1553],\n",
            "         [-0.3683, -0.1869, -0.4032,  ..., -0.1569,  0.0446, -0.1049],\n",
            "         [-0.3760, -0.2513, -0.3373,  ..., -0.1399,  0.0418, -0.2199],\n",
            "         ...,\n",
            "         [-0.2296,  0.3407, -0.1926,  ..., -0.2464, -0.0885, -0.2905],\n",
            "         [-0.2521,  0.3283, -0.0810,  ..., -0.2422, -0.0836, -0.2940],\n",
            "         [-0.3297,  0.2685, -0.1152,  ..., -0.3087,  0.0198, -0.1060]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diarization (in progress, don't run)"
      ],
      "metadata": {
        "id": "LcBgvjpVMI86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")"
      ],
      "metadata": {
        "id": "7in2_Z_VObZx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"general\" #\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)"
      ],
      "metadata": {
        "id": "PHb5MoleNBUs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UcgjQ_-b2adZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}