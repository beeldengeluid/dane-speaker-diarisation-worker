{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LcBgvjpVMI86"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoZ59rEP-He6",
        "outputId": "57f207a6-14e9-4e24-ee15-7b3d926d8c6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2k3muZc9n5-",
        "outputId": "a364e626-2f1b-4077-d58c-f45d70b240fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting working directory\n",
        "%cd /content/drive/MyDrive/master/data_internship"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1M0LYp59pP1",
        "outputId": "9880c11a-ed7c-4a3a-afdc-1f639e9a8182"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/master/data_internship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvMVl1iP9xLH",
        "outputId": "a72dddf3-93d0-43d2-93a2-9c731f95b815"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/master/data_internship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyannote.audio\n",
        "!pip install pyannote.core\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install faster-whisper\n",
        "!pip install --no-build-isolation nemo_toolkit[asr]==1.22.0\n",
        "!pip install ffmpeg\n",
        "#!pip install wget"
      ],
      "metadata": {
        "id": "pqA8eiMW5Gay"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hkKwlEXj2Res",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5df334-6a01-46da-ba01-b594234b85a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "[NeMo W 2024-03-05 14:15:44 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
          ]
        }
      ],
      "source": [
        "#segment\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import torch, torchaudio\n",
        "import ffmpeg\n",
        "import os\n",
        "#import librosa\n",
        "\n",
        "#embed\n",
        "from faster_whisper import WhisperModel\n",
        "#from segment import segment_audio\n",
        "#from embed import embed_audio\n",
        "import tensorflow as tf\n",
        "\n",
        "#diarize\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "import wget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "h65tbh7NMW3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio = Audio(mono='downmix') #add parsing arg (for nemo we need mono)\n",
        "\n",
        "def segmentation(trans_segment,\n",
        "            waveform,\n",
        "            #video,\n",
        "            sr,\n",
        "            seg_num,\n",
        "            segment_audio=True,\n",
        "            segment_video=False,\n",
        "            save_into_file=False):\n",
        "  '''\n",
        "  Given a start and end timestamp from the transcription, it crops an audio file\n",
        "  or segment returned as a torch tensor. Moreover, if save_into_file=True, it\n",
        "  saves the segment into a .wav and .pt file. For default, turned off as it\n",
        "  consumes storage memory.\n",
        "  '''\n",
        "\n",
        "  if segment_audio: #kathleen needs: .wav segs, .pt segs\n",
        "    #segmenting\n",
        "    start = trans_segment.start\n",
        "    end = trans_segment.end\n",
        "    clip = Segment(start, end)\n",
        "    audio_segment, sr = audio.crop(waveform, clip) #as a tensor\n",
        "\n",
        "    if save_into_file:\n",
        "      #extracting file name and extension\n",
        "      name, extension = os.path.splitext(waveform)\n",
        "      if not os.path.exists(\"%s_segments\" % (name)):\n",
        "        os.makedirs(\"%s_segments\" % (name))\n",
        "\n",
        "      #saving segment into .wav\n",
        "      torchaudio.save(\"./%s_segments/%s_seg_%x%s\" % (name, name, seg_num, extension),\n",
        "                          audio_segment, sr, format=\"wav\")\n",
        "\n",
        "      #saving segment into .pt\n",
        "      torch.save(audio_segment, \"./%s_segments/%s_seg_%x.pt\" % (name, name, seg_num), _use_new_zipfile_serialization=False)\n",
        "\n",
        "    return audio_segment\n",
        "\n",
        "    #if segment_video: #kathleen needs: .mp4 segs\n",
        "        #ffmpeg -i video -ss 00:01:00 -to 00:02:00 -c copy output_segmented_video.mp4"
      ],
      "metadata": {
        "id": "9tu_-TNH2aUw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "vnV0RBAUMZmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "  '''\n",
        "  Loads feature extraction model, configuration and weights.\n",
        "  '''\n",
        "\n",
        "  # load the pre-trained checkpoints\n",
        "  model_checkpoint_loc = 'WavLM-Large.pt'\n",
        "  checkpoint = torch.load(model_checkpoint_loc) #should be .pt file\n",
        "  cfg = WavLMConfig(checkpoint['cfg'])\n",
        "  model = WavLM(cfg)\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  model.eval()\n",
        "  return model, cfg\n",
        "\n",
        "def embed_audio(model, cfg, audio_segment, last_layer=True):\n",
        "  '''\n",
        "  Generates embeddings for the given audio file or segment. Extracts the\n",
        "  representation of the last layer (last_layer=True), or the representation from\n",
        "  each layer and performs a weighted sum (last_layer=False).\n",
        "  '''\n",
        "\n",
        "  if last_layer:\n",
        "    if cfg.normalize:\n",
        "        wav_input_16khz = torch.nn.functional.layer_norm(audio_segment, audio_segment.shape)\n",
        "    rep = model.extract_features(wav_input_16khz)[0]\n",
        "\n",
        "  if last_layer == False:\n",
        "    if cfg.normalize:\n",
        "      wav_input_16khz = torch.nn.functional.layer_norm(wav_input_16khz , wav_input_16khz.shape)\n",
        "    rep, layer_results = model.extract_features(wav_input_16khz, output_layer=model.cfg.encoder_layers, ret_layer_results=True)[0]\n",
        "    layer_reps = [x.transpose(0, 1) for x, _ in layer_results]\n",
        "\n",
        "  return rep"
      ],
      "metadata": {
        "id": "rjV3BUCO2aWm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read from .py file to import functions\n",
        "#import sys\n",
        "#sys.path.append('/content/drive/MyDrive/master/data_internship')\n",
        "#from WavLM import WavLM, WavLMConfig"
      ],
      "metadata": {
        "id": "L_oVIr8-I2oC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from WavLM import WavLM, WavLMConfig\n",
        "#loading feature extraction model\n",
        "feat_model, cfg = load_model()"
      ],
      "metadata": {
        "id": "-JLk6vYsI_iK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "3fFe6J4OMfI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster-whisper Transcription"
      ],
      "metadata": {
        "id": "nx19gQrbMrv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = \"large-v3\"\n",
        "\n",
        "# Run on GPU with FP16\n",
        "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "# or run on GPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "# or run on CPU with INT8\n",
        "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
        "\n",
        "audioin = \"testaudio.wav\"\n",
        "\n",
        "segments, info = model.transcribe(audioin, beam_size=5)\n",
        "\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))"
      ],
      "metadata": {
        "id": "5sVZEmil2aYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12951909-3f53-4982-d6b3-308655369723"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:libav.wav:Ignoring maximum wav data size, file may be invalid\n",
            "WARNING:libav.wav:Estimating duration from bitrate, this may be inaccurate\n",
            "WARNING:libav.adpcm_ms:Multiple frames in a packet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language 'en' with probability 0.913472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment and embed"
      ],
      "metadata": {
        "id": "yB2TstG9M5XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- PARAMS ---#\n",
        "#sampling rate\n",
        "sr = 16000\n",
        "#segment number initialization\n",
        "seg_num = 0\n",
        "\n",
        "for segment in segments:\n",
        "    #print(segment.text)\n",
        "    #calling segmentation creates\n",
        "    seg = segmentation(segment, audioin, sr, seg_num)\n",
        "    seg_num+=1\n",
        "    emb = embed_audio(feat_model, cfg, seg)\n",
        "    print(emb)"
      ],
      "metadata": {
        "id": "6ftM5YYh2aax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diarization (in progress, don't run)"
      ],
      "metadata": {
        "id": "LcBgvjpVMI86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")"
      ],
      "metadata": {
        "id": "7in2_Z_VObZx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"general\" #\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)"
      ],
      "metadata": {
        "id": "PHb5MoleNBUs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UcgjQ_-b2adZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}